{"cells":[{"cell_type":"markdown","metadata":{"id":"mq_q8VZnieFO"},"source":["# Text Generation with RNN\n","Recurrent Neural Network used to analyze textual or visual data that is sequential.\n"]},{"cell_type":"markdown","metadata":{"id":"FLK4SVH8kQ5F"},"source":["## Importing Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mqjzhRbiiYGd"},"outputs":[],"source":["# Import the tensorflow library.\n","import tensorflow as tf\n","\n","# Import the numpy library.\n","import numpy as np\n","\n","# Import the operating system.\n","import os\n","\n","# Import the time library.\n","import time"]},{"cell_type":"markdown","metadata":{"id":"c3ey93k6k7lu"},"source":["## Reading the Recipes File"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LghQC2cLoqQI","colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"status":"error","timestamp":1739755766805,"user_tz":300,"elapsed":505,"user":{"displayName":"Viraaj Singh","userId":"13220583761423652252"}},"outputId":"6a38746f-89fd-41f2-b8e9-45d5e0d189a2"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'recipes.txt'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-ce1675d42afc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Open the file in your program.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Print the first 250 characters of the file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'recipes.txt'"]}],"source":["# Add the \"recipes.txt\" file path from your student folder.\n","path_to_file=(\"recipes.txt\")\n","\n","# Open the file in your program.\n","text=open(path_to_file,\"rb\").read().decode(encoding=\"utf8\")\n","\n","# Print the first 250 characters of the file.\n","print(text[:250])"]},{"cell_type":"markdown","metadata":{"id":"G4n6ySe1o70K"},"source":["## Length and Unique Characters\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JQdFnSP3o3SG","outputId":"9ce39491-e907-4702-8840-87c114089531"},"outputs":[{"name":"stdout","output_type":"stream","text":["2390691\n"]}],"source":["# Print the length of the text.\n","print(len(text))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WyoBemdN0R-I","outputId":"a978cd1b-6780-4560-eac8-979176ccd6ec"},"outputs":[{"name":"stdout","output_type":"stream","text":["['\\t', '\\n', ' ', '!', '\"', '#', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '\\\\', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '~']\n"]}],"source":["# Create a variable called vocab and sort the text into a set.\n","vocab=sorted(set(text))\n","\n","# Print the vocab set.\n","print(vocab)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gJaXiHDz0aF3","outputId":"059ccf96-f2af-490e-d1e2-94305c123ce8"},"outputs":[{"data":{"text/plain":["86"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# Print out the length of the vocab set.\n","len(vocab)"]},{"cell_type":"markdown","metadata":{"id":"_ZJ5AvP20dip"},"source":["# Vectorizing\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_OJPaQYy0fBT"},"outputs":[],"source":["# Create a list called example_texts and add some random text.\n","example_texts=[\"the quick brown fox\", \"jumped\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"beSUuwm27NXT","outputId":"a38efadf-518a-4277-87ec-64a3a1d0b088"},"outputs":[{"name":"stdout","output_type":"stream","text":["<tf.RaggedTensor [[b't', b'h', b'e', b' ', b'q', b'u', b'i', b'c', b'k', b' ', b'b', b'r',\n","  b'o', b'w', b'n', b' ', b'f', b'o', b'x']                              ,\n"," [b'j', b'u', b'm', b'p', b'e', b'd']]>\n"]}],"source":["# Create a variable called chars, and split the text to tokenize it.\n","chars=tf.strings.unicode_split(example_texts, input_encoding=\"UTF-8\")\n","\n","# Print out the characters.\n","print(chars)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WypD23CG9cwi","outputId":"a9c238f1-eaa9-4565-a6fe-81f329876983"},"outputs":[{"name":"stdout","output_type":"stream","text":["<tf.RaggedTensor [[79, 67, 64, 3, 76, 80, 68, 62, 70, 3, 61, 77, 74, 82, 73, 3, 65, 74, 83],\n"," [69, 80, 72, 75, 64, 63]]>\n"]}],"source":["# Create a layer called ids_from_chars that will convert characters to ids using the vocab set.\n","ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)\n","\n","# Create a variable called ids and get the ids from the layer that was created above.\n","ids = ids_from_chars(chars)\n","\n","# Print out the ids from the layer.\n","print(ids)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Trkzf1LhKLto","outputId":"31fde7c1-24d3-42ef-84a9-5ac0e493709a"},"outputs":[{"name":"stdout","output_type":"stream","text":["<tf.RaggedTensor [[b't', b'h', b'e', b' ', b'q', b'u', b'i', b'c', b'k', b' ', b'b', b'r',\n","  b'o', b'w', b'n', b' ', b'f', b'o', b'x']                              ,\n"," [b'j', b'u', b'm', b'p', b'e', b'd']]>\n"]}],"source":["# Create a layer called chars_from_ids that will convert ids to characters using the vocab set.\n","chars_from_ids = tf.keras.layers.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n","\n","# Create a variable called chars and get the characters from the layer that was created above.\n","chars = chars_from_ids(ids)\n","\n","# Print out the characters from the layer.\n","print(chars)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5Xnip64AKZnn","outputId":"fcd54c56-73c8-427b-c8ba-8a11f03a684f"},"outputs":[{"name":"stdout","output_type":"stream","text":["tf.Tensor([b'the quick brown fox' b'jumped'], shape=(2,), dtype=string)\n"]}],"source":["# Create a function called text_from_ids to convert the ids into text.\n","def text_from_ids(ids):\n","    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n","\n","print(text_from_ids(ids))"]},{"cell_type":"markdown","metadata":{"id":"xI639tLULMUk"},"source":["# Creating the Training Data\n"]},{"cell_type":"markdown","metadata":{"id":"2EqnADfxLNMB"},"source":["## Create the Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m3HE09f9LM8q","outputId":"5e2ff073-de1b-471a-b172-38f515062f08"},"outputs":[{"name":"stdout","output_type":"stream","text":["tf.Tensor([45 74 15 ... 16  2  2], shape=(2390691,), dtype=int64)\n"]}],"source":["# Create a variable called all_ids and convert the text into ids.\n","all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n","\n","# Print out all the ids and shape.\n","print(all_ids)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hQ439F1oNGnx"},"outputs":[],"source":["# Create a variable called ids_dataset use tensorflow to create a dataset.\n","ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W7FGrdb4OA4X","outputId":"81b7377a-d319-412d-8f10-ed90702e5aed"},"outputs":[{"name":"stdout","output_type":"stream","text":["N\n","o\n","-\n","B\n","a\n","k\n","e\n"," \n","N\n","u\n"]}],"source":["# Create a for loop that iterates through 10 pieces of the ids dataset using the .take(n) function.\n","for ids in ids_dataset.take(10):\n","    print(chars_from_ids(ids).numpy().decode('utf-8'))"]},{"cell_type":"markdown","metadata":{"id":"3dHHGIwoORm7"},"source":["## Divide Text into Sequences\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A0v4pY0ePdAf","outputId":"163e57cb-d23a-4382-b930-b0dd36aff33b"},"outputs":[{"name":"stdout","output_type":"stream","text":["tf.Tensor(\n","[b'N' b'o' b'-' b'B' b'a' b'k' b'e' b' ' b'N' b'u' b't' b' ' b'C' b'o'\n"," b'o' b'k' b'i' b'e' b's' b'\\n' b'1' b' ' b'c' b'.' b' ' b'f' b'i' b'r'\n"," b'm' b'l' b'y' b' ' b'p' b'a' b'c' b'k' b'e' b'd' b' ' b'b' b'r' b'o'\n"," b'w' b'n' b' ' b's' b'u' b'g' b'a' b'r' b'\\n' b' ' b'1' b'/' b'2' b' '\n"," b'c' b'.' b' ' b'e' b'v' b'a' b'p' b'o' b'r' b'a' b't' b'e' b'd' b' '\n"," b'm' b'i' b'l' b'k' b'\\n' b' ' b'1' b'/' b'2' b' ' b't' b's' b'p' b'.'\n"," b' ' b'v' b'a' b'n' b'i' b'l' b'l' b'a' b'\\n' b' ' b'1' b'/' b'2' b' '\n"," b'c' b'.' b' '], shape=(101,), dtype=string)\n"]}],"source":["# Create a variable called seq_length and set the value to the length of your sequences, 100.\n","seq_length = 100\n","\n","# Set how many examples you want to run per epoch\n","examples_per_epoch = len(text)//(seq_length+1)\n","\n","# Create a variable called sequences and use the batch function to divide the data into sequences.\n","sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n","\n","# Create a for loop, and use the .take() function to take 1 sequence from sequences.\n","for seq in sequences.take(1):\n","    print(chars_from_ids(seq))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6uimumbxSISv","outputId":"59597be0-f132-4038-81e5-82decdb0d826"},"outputs":[{"name":"stdout","output_type":"stream","text":["tf.Tensor(b'No-Bake Nut Cookies\\n1 c. firmly packed brown sugar\\n 1/2 c. evaporated milk\\n 1/2 tsp. vanilla\\n 1/2 c. ', shape=(), dtype=string)\n","tf.Tensor(b'broken nuts (pecans)\\n 2 Tbsp. butter or margarine\\n 3 1/2 c. bite size shredded rice biscuitsIn a heav', shape=(), dtype=string)\n","tf.Tensor(b'y 2-quart saucepan, mix brown sugar, nuts, evaporated milk and butter or margarine.\\n Stir over medium', shape=(), dtype=string)\n","tf.Tensor(b' heat until mixture bubbles all over top.\\n Boil and stir 5 minutes more. Take off heat.\\n Stir in vani', shape=(), dtype=string)\n","tf.Tensor(b'lla and cereal; mix well.\\n Using 2 teaspoons, drop and shape into 30 clusters on wax paper.\\n Let stan', shape=(), dtype=string)\n"]}],"source":["# Create a for loop using the .take() function to take 5 sequences.\n","for seq in sequences.take(5):\n","    print(text_from_ids(seq))"]},{"cell_type":"markdown","metadata":{"id":"j-VsRXLgSosL"},"source":["## Create Input and Target Pairs\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hwPvoLn0SuRi"},"outputs":[],"source":["# Create a function called split_input_target that will split a sequence into input text and target text.\n","def split_input_target(sequence):\n","    input_text = sequence[:-1]\n","    target_text = sequence[1:]\n","    return input_text, target_text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FonQVdpGTik1","outputId":"f0770f1f-653f-4e43-e449-45a33b2f668e"},"outputs":[{"name":"stdout","output_type":"stream","text":["(['i', ' ', 'l', 'o', 'v', 'e', ' ', 'i', 'd', 't', 'e', 'c'], [' ', 'l', 'o', 'v', 'e', ' ', 'i', 'd', 't', 'e', 'c', 'h'])\n"]}],"source":["# Create a variable called example_text and add some sample text.\n","example_text = \"i love idtech\"\n","\n","# Print the split input and target lists.\n","print(split_input_target(list(example_text)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZskNkEx_UW39"},"outputs":[],"source":["# Use the split_input_target to create a dataset of the input and target text for each sequence.\n","dataset = sequences.map(split_input_target)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dL2bURAdU9sH","outputId":"9b17cc98-f078-46b4-b9c3-4d204aaad2fc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Input : b'No-Bake Nut Cookies\\n1 c. firmly packed brown sugar\\n 1/2 c. evaporated milk\\n 1/2 tsp. vanilla\\n 1/2 c.'\n","Target: b'o-Bake Nut Cookies\\n1 c. firmly packed brown sugar\\n 1/2 c. evaporated milk\\n 1/2 tsp. vanilla\\n 1/2 c. '\n"]}],"source":["# Create a for loop that iterates through the dataset to get 1 input example and target example.\n","for input_example, target_example in dataset.take(1):\n","    print(\"Input :\", text_from_ids(input_example).numpy())\n","    print(\"Target:\", text_from_ids(target_example).numpy())"]},{"cell_type":"markdown","metadata":{"id":"aQVCITVYU-7a"},"source":["# Final Touches\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b6G36amQVCDt","outputId":"30ecfd97-6865-4e96-e05b-414ed04924e1"},"outputs":[{"name":"stdout","output_type":"stream","text":["<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>\n"]}],"source":["# Set a batch size to 64 so that the data can be entered into the network.\n","BATCH_SIZE = 64\n","\n","# Set a buffer size to 1000 so that you can shuffle the dataset without using too much memory.\n","BUFFER_SIZE = 10000\n","\n","# Prepare your dataset to train the network.\n","dataset = (\n","    dataset\n","    .shuffle(BUFFER_SIZE)\n","    .batch(BATCH_SIZE, drop_remainder=True)\n","    .prefetch(tf.data.experimental.AUTOTUNE))\n","\n","# Print the final prepared dataset.\n","print(dataset)"]},{"cell_type":"markdown","metadata":{"id":"olKvSvI4AFGC"},"source":["# Building Network\n"]},{"cell_type":"markdown","metadata":{"id":"FQNswAnvDcb1"},"source":["## Network Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zoVlZplWDslQ"},"outputs":[],"source":["# Create a variable called vocab_size and set the value as the length of the vocab.\n","vocab_size = len(vocab)\n","\n","# Create a variable called embedding_dim and set the value as the 256.\n","embedding_dim = 256\n","\n","# Create a variable called rnn_units and set the value as 1024 to get the network started.\n","rnn_units = 1024"]},{"cell_type":"markdown","metadata":{"id":"ZuKY46Oi5-IB"},"source":["## Model Breakdown\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z5-kZYKQ6Bvx"},"outputs":[],"source":["# Create a class called MyModel to define the model with two functions called init and call.\n","class MyModel(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_dim, rnn_units):\n","        super().__init__(self)\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","        self.gru = tf.keras.layers.GRU(rnn_units,\n","                                       return_sequences=True,\n","                                       return_state=True)\n","        self.dense = tf.keras.layers.Dense(vocab_size)\n","\n","    def call(self, inputs, states=None, return_state=False, training=False):\n","        x = inputs\n","        x = self.embedding(x, training=training)\n","        if states is None:\n","            states = self.gru.get_initial_state(x)\n","        x, states = self.gru(x, initial_state=states, training=training)\n","        x = self.dense(x, training=training)\n","\n","        if return_state:\n","            return x, states\n","        else:\n","            return x"]},{"cell_type":"markdown","metadata":{"id":"j7wMf8dn94p6"},"source":["## Model Design\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uIBH8Cwv96ZX","outputId":"61080240-e769-410e-9d3f-9bf28919e086"},"outputs":[{"ename":"TypeError","evalue":"Layer.__init__() takes 1 positional argument but 2 were given","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[24], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create a model variable using the MyModel class.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model\u001b[38;5;241m=\u001b[39m\u001b[43mMyModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mids_from_chars\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_vocabulary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrnn_units\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrnn_units\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[1;32mIn[23], line 4\u001b[0m, in \u001b[0;36mMyModel.__init__\u001b[1;34m(self, vocab_size, embedding_dim, rnn_units)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, vocab_size, embedding_dim, rnn_units):\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mEmbedding(vocab_size, embedding_dim)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgru \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mGRU(rnn_units,\n\u001b[0;32m      7\u001b[0m                                    return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      8\u001b[0m                                    return_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\models\\model.py:156\u001b[0m, in \u001b[0;36mModel.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m     functional\u001b[38;5;241m.\u001b[39mFunctional\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 156\u001b[0m     Layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","\u001b[1;31mTypeError\u001b[0m: Layer.__init__() takes 1 positional argument but 2 were given"]}],"source":["# Create a model variable using the MyModel class.\n","model=MyModel(vocab_size=len(ids_from_chars.get_vocabulary()), embedding_dim=embedding_dim, rnn_units=rnn_units)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VDw6B8SH-AIs"},"outputs":[],"source":["# Create a for loop to set the batch size, sequence length, and vocab size of an example from the dataset.\n","for input_example_batch, target_example_batch in dataset.take(1):\n","    example_batch_predictions=model(input_example_batch)\n","    print(example_batch_predictions.shape, \" (batch_size, sequence_length, vocab_size)\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GO3BjdZk-pc0"},"outputs":[],"source":["# Print the summary of the model.\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"BQeLLOay-vjx"},"source":["## Random Example\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X-0aSUj6-xrT"},"outputs":[],"source":["# Add in sampled indices to your code to see how the program functions.\n","sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n","sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n","\n","\n","# Print out the input and next character predictions for the sample.\n","print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n","print(\"\\nNext Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"]},{"cell_type":"markdown","metadata":{"id":"lxQjA7RY-8FY"},"source":["# Training\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cGxSd3iI-93b"},"outputs":[],"source":["# Set the loss function.\n","loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n","\n","# Get an example batch mean loss.\n","example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n","\n","\n","# Add a print statement to print the shape of example_batch_predictions.\n","print(example_batch_predictions.shape)\n","print(\"^ # (batch_size, sequence_length, vocab_size)\")\n","\n","\n","# Add a print statement to print example_batch_mean_loss.\n","print(example_batch_mean_loss)\n","\n","\n","# Print the exponential of the average loss.\n","print(\"Exponential of average loss: \", tf.exp(example_batch_mean_loss).numpy())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IgjpfPfz_iK2"},"outputs":[],"source":["# Set the network's optimizer and loss.\n","model.compile(optimizer='adam', loss=loss)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5l8W2zr-_pFf"},"outputs":[],"source":["# Add some checkpoints to keep track of the training process.\n","checkpoint_dir = './training_checkpoints'\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n","checkpoint_callback = tf.keras.callbacks.ModelCheckpoint( filepath=checkpoint_prefix, save_weights_only=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bPdscQ7v_1w8"},"outputs":[],"source":["# Set the number of epochs to 20.\n","epochs=20"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D2ixHk_e_3JH"},"outputs":[],"source":["# Add the fit function and set the input data for the model.\n","history=model.fit(dataset, epochs=epochs, callbacks=[checkpoint_callback])"]},{"cell_type":"markdown","metadata":{"id":"U1WRDvqEEsBA"},"source":["# Generate Text\n"]},{"cell_type":"markdown","metadata":{"id":"MRxAJuisnYS1"},"source":["## Define a One Step Model & Generate One-Step Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PZVVRIlSEtWJ"},"outputs":[],"source":["# Create a class called OneStep that uses the model to predict new text.\n","class OneStep(tf.keras.Model):\n","    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n","        super().__init__()\n","        self.temperature = temperature\n","        self.model = model\n","        self.chars_from_ids = chars_from_ids\n","        self.ids_from_chars = ids_from_chars\n","\n","        skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n","        sparse_mask = tf.SparseTensor(\n","          values=[-float('inf')]*len(skip_ids),\n","          indices=skip_ids,\n","          dense_shape=[len(ids_from_chars.get_vocabulary())])\n","\n","        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n","\n","    @tf.function\n","    def generate_one_step(self, inputs, states=None):\n","        input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n","        input_ids = self.ids_from_chars(input_chars).to_tensor()\n","\n","        predicted_logits, states = self.model(inputs=input_ids, states=states, return_state=True)\n","        predicted_logits = predicted_logits[:, -1, :]\n","        predicted_logits = predicted_logits/self.temperature\n","        predicted_logits = predicted_logits\n","\n","        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n","        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n","        predicted_chars = self.chars_from_ids(predicted_ids)\n","\n","        return predicted_chars, states"]},{"cell_type":"markdown","metadata":{"id":"_Gu-DOlvoU1H"},"source":["## Model Setup\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WUA3pH_DoYOD"},"outputs":[],"source":["# Generate a \"one step\" model using the OneStep class.\n","one_step_model=OneStep(model, chars_from_ids, ids_from_chars)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c_5nfE68pAFO"},"outputs":[],"source":["# Initialize the states to None.\n","states=None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_xBehVEFpBEp"},"outputs":[],"source":["# Create a variable called next_char and add the text to start the network.\n","next_char=tf.constant([\" \"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DppP-jaYpBKc"},"outputs":[],"source":["# Create a list called result and add the next_char.\n","result= [next_char]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EuJzS9tupRqA"},"outputs":[],"source":["# Create a for loop with the amount of characters that you want generate (1000 is a good number to start with).\n","for n in range (1000):\n","    next_char, states =  one_step_model.generate_one_step(next_char, states=states)\n","    result.append(next_char)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MYsC8d3cpck4"},"outputs":[],"source":["# Take the results from the for loop and join it together to create a string.\n","result = tf.strings.join(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hk9CDsYgpeou"},"outputs":[],"source":["# Print the result to see the joined string.\n","print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hqioeEWopq4k"},"outputs":[],"source":["# Adjust your network below.\n"]},{"cell_type":"markdown","metadata":{"id":"WE5tO1m0to68"},"source":["## Exporting Model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xofZb-aYttBg"},"outputs":[],"source":["# Export your model.\n","tf.saved_model.save(one_step_model, 'one_step')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UsTNlOtTt7tL"},"outputs":[],"source":["# Reload the model from the one_step folder.\n","data=tf.saved_model.load(\"one_step\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DE-1ghY4gDFv"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"}},"nbformat":4,"nbformat_minor":0}